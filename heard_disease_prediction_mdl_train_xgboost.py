# -*- coding: utf-8 -*-
"""heard disease prediction mdl train xgboost

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1o1JU3J9EObsSIWiEhWBdKTmZFifevPY9
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.preprocessing import StandardScaler
from xgboost import XGBClassifier

# Load your uploaded CSV
df = pd.read_csv("heart_disease_data.csv")
df.head()

# Check for missing values
print(df.isnull().sum())

# Separate features and target
X = df.drop("target", axis=1)
y = df["target"]

# Standardize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split dataset
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Select user-friendly features
selected_features = ["cp", "thalach", "sex", "age"]

# Prepare feature matrix and target vector
X = df[selected_features]
y = df["target"]

# Scale features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split data
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Train Random Forest model
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Predict and evaluate
y_pred_rf = rf_model.predict(X_test)
accuracy_rf = accuracy_score(y_test, y_pred_rf)
report_rf = classification_report(y_test, y_pred_rf)

print("Random Forest Accuracy:", accuracy_rf)
print(report_rf)

# 0. (If not already) install dependencies
!pip install xgboost scikit-learn pandas numpy matplotlib seaborn --quiet

# 1. Imports
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from xgboost import XGBClassifier
import joblib

# 2. (Skip if df already exists) Load your data
# df = pd.read_csv("heart_disease_data.csv")

# 3. Select the 4 user-friendly features + target
features = ["cp", "thalach", "sex", "age"]
X = df[features]
y = df["target"]

# 4. Scale numeric inputs
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 5. Train/test split (stratify to keep class balance)
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.20, random_state=42, stratify=y
)

# 6. Configure XGBoost + hyperparameter grid
xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', n_jobs=-1)
param_grid = {
    "max_depth":    [3, 5, 7],
    "learning_rate":[0.01, 0.1],
    "n_estimators": [50, 100],
    "subsample":    [0.8, 1.0]
}
grid = GridSearchCV(
    estimator=xgb,
    param_grid=param_grid,
    cv=4,
    scoring="accuracy",
    n_jobs=-1,
    verbose=2
)

# 7. Fit the grid search
grid.fit(X_train, y_train)

# 8. Examine best params & evaluate
print("▶ Best hyperparameters:", grid.best_params_)
best_xgb = grid.best_estimator_

y_pred = best_xgb.predict(X_test)
acc = accuracy_score(y_test, y_pred)
print(f"\nTest Accuracy: {acc:.4f}\n")
print(classification_report(y_test, y_pred))

# 9. Plot confusion matrix
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
plt.title("XGBoost Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

# 10. Save model + scaler for later (Android integration)
joblib.dump(best_xgb, "xgb_heart_model.pkl")
joblib.dump(scaler, "scaler.pkl")

!pip install onnxmltools xgboost onnxruntime skl2onnx --quiet

import onnxmltools
from skl2onnx import convert_sklearn
from skl2onnx.common.data_types import FloatTensorType

# Define input type for 4 input features
initial_type = [('float_input', FloatTensorType([None, 4]))]

# Convert XGBoost model
onnx_model = convert_sklearn(best_xgb, initial_types=initial_type)

# Save to file
with open("xgb_heart_model.onnx", "wb") as f:
    f.write(onnx_model.SerializeToString())

print("✅ Model converted to ONNX format")

# 1. Install dependencies
!pip install onnxmltools xgboost onnx onnxruntime --quiet

import onnxmltools

# Convert XGBoost model to ONNX
onnx_model = onnxmltools.convert_xgboost(best_xgb,
                                         initial_types=[("float_input", onnxmltools.convert.common.data_types.FloatTensorType([None, 4]))])

# Save to file
with open("xgb_heart_model.onnx", "wb") as f:
    f.write(onnx_model.SerializeToString())

print("✅ XGBoost model successfully converted to ONNX format!")

from google.colab import files
files.download("xgb_heart_model.onnx")
files.download("scaler.pkl")